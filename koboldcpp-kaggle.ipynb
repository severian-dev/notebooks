{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/severiandev/koboldcpp-notebook?scriptVersionId=265512637\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# KoboldCPP on Kaggle\n\nHost models up to 32B with this notebook. A slightly more organized version of [Divine's notebook here](https://www.kaggle.com/code/divinesinner/koboldcpp-guide-in-comment/notebook).\n\n### Relevant links\n- [Divine's guide](https://www.kaggle.com/code/divinesinner/koboldcpp-guide-in-comment/comments#3102042)\n- [Hibiki's model recommendations on the unofficial Colab fork](https://colab.research.google.com/drive/1l_wRGeD-LnRl3VtZHDc7epW_XW0nJvew#scrollTo=pf4AQOYgTB2d)\n- [nyxkrage's VRAM Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)\n- [Myscell's local model recommendations](https://rentry.org/anathem)\n- [Pepper's local model reviews](https://www.notion.so/playwithpepper/1f392d900248803f86c2c51c73f92a0b?v=1f392d90024880d59e33000cc1b15175)\n- [Naen's Kaggle guide, if not a bit outdated](https://rentry.org/GodsGreatestKaggles)\n- [tokens per sec, per model class/quant by knightess](https://gist.github.com/severian-dev/f7712407f239aeceb3a56bc75ba7d81b?permalink_comment_id=5765568#gistcomment-5765568)\n- ~~[trashpanda-org on HF, just because I can](https://huggingface.co/trashpanda-org)~~\n\n### Changelog\n- Oct 3, 2025: revised model list (thanks, Myscell!)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## Before you do anything else\n\nMake sure you've done the following (refer to Naen's guide above if unsure how):\n\n1. Sign up for a **Kaggle** account with a **verified phone number**. You might be asked to do facial verification via Persona.\n2. Sign up for an **ngrok** account and acquire an **authtoken** [(signup/login here)](https://ngrok.com/).\n3. Add this ngrok token as a secret: In the **`Add-ons`** menu above -> **`Secrets`**, click on the **`Add Secret`** button. Label is `ngrok-auth`, value is your token. Click **`Save`**.\n4. In the **`Settings`** menu above -> **`Accelerator`**, select **`GPU T4x2`**, and click on **`Turn on Internet`**\n\n## How to use this notebook\n\n- Run each cell one by one if you've done all the setup steps described above.\n- If you have the max tokens setting as 0 in any frontend that allows such a thing, default max token is set to 2048 below.\n- `https://xxxxxxxx.ngrok-free.app/v1/chat/completions` is the proxy URL format to use, see ngrok cell below. Model name and API Key you can just put a `.` in it.\n\n### Want to change models?\n\n- Simply rerun the Download model and Launch Kobold cells after selecting a new one.\n\n### Want to change sampler setting options / instruct template?\n\n- Automatically applies, no need to rerun any cell.\n\n### The model I want to run is not on the list below\n\n- You need a model quant URL to download and set up with. Too long and disruptive for a guide to be here, so just check out [this model quant selection guide](https://rentry.org/severian) for more details, including some screenshots.\n\n### What's the music file for?\n\n- It's recommended to use the audio file below to keep Kaggle from killing the runtime after 49 minutes. Run the cell first, before playing the audio file (note how these are two different steps.) You'll need it on mobile too, I assume.\n\n### Issues?\n\nIf you're sure it's not some skill issue on your end, either DM me on Discord: @lyseverian, or send a pull request at https://github.com/severian-dev/notebooks if you want to add more models to the list.","metadata":{}},{"cell_type":"code","source":"%%html\n<h5>Press play on the music player to keep the tab alive (Uses only 13MB of data)</h5>\n<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2025-09-17T09:06:17.265168Z","iopub.status.busy":"2025-09-17T09:06:17.264782Z","iopub.status.idle":"2025-09-17T09:06:17.271257Z","shell.execute_reply":"2025-09-17T09:06:17.270426Z","shell.execute_reply.started":"2025-09-17T09:06:17.265145Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download Kobold","metadata":{}},{"cell_type":"code","source":"!curl -fLo koboldcpp https://github.com/LostRuins/koboldcpp/releases/download/v1.98.1/koboldcpp-linux-x64 && chmod +x koboldcpp","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-09-23T17:14:08.086393Z","iopub.status.busy":"2025-09-23T17:14:08.086153Z","iopub.status.idle":"2025-09-23T17:14:52.778249Z","shell.execute_reply":"2025-09-23T17:14:52.777601Z","shell.execute_reply.started":"2025-09-23T17:14:08.08637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Settings (model, samplers)\n\nAfter running this cell, make changes to the settings below (select model, check context and max token, sampler settings) before moving on to the cells after this one.\n\nThe fields look like shit rn, I'll just improve it later.","metadata":{}},{"cell_type":"code","source":"# Model list, settings such as context, max tokens, instruct preset, and advanced settings\nfrom IPython.display import display\nimport ipywidgets as widgets\n\npremade_instruct = {\n    \"alpaca\": {\n        \"system_start\": \"\\n### Input: \",\n        \"system_end\": \"\",\n        \"user_start\": \"\\n### Instruction: \",\n        \"user_end\": \"\",\n        \"assistant_start\": \"\\n### Response: \",\n        \"assistant_end\": \"\",\n    },\n    \"vicuna\": {\n        \"system_start\": \"\\nSYSTEM: \",\n        \"system_end\": \"\",\n        \"user_start\": \"\\nUSER: \",\n        \"user_end\": \"\",\n        \"assistant_start\": \"\\nASSISTANT: \",\n        \"assistant_end\": \"\",\n    },\n    \"llama-3\": {\n        \"system_start\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n        \"system_end\": \"<|eot_id|>\",\n        \"user_start\": \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n        \"user_end\": \"<|eot_id|>\",\n        \"assistant_start\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n        \"assistant_end\": \"<|eot_id|>\",\n    },\n    \"chatml\": {\n        \"system_start\": \"<|im_start|>system\",\n        \"system_end\": \"<|im_end|>\",\n        \"user_start\": \"<|im_start|>user\",\n        \"user_end\": \"<|im_end|>\",\n        \"assistant_start\": \"<|im_start|>assistant\",\n        \"assistant_end\": \"<|im_end|>\",\n    },\n    \"command-r\": {\n        \"system_start\": \"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\",\n        \"system_end\": \"<|END_OF_TURN_TOKEN|>\",\n        \"user_start\": \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\",\n        \"user_end\": \"<|END_OF_TURN_TOKEN|>\",\n        \"assistant_start\": \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\",\n        \"assistant_end\": \"<|END_OF_TURN_TOKEN|>\",\n    },\n    \"mistral\":  {\n      \"system_start\": \"\",\n      \"system_end\": \"\",\n      \"user_start\": \"[INST] \",\n      \"user_end\": \"\",\n      \"assistant_start\": \" [/INST]\",\n      \"assistant_end\": \"</s> \"\n    },\n    \"mistral-v3-tekken\":  {\n      \"system_start\": \"[INST]\",\n      \"system_end\": \"[/INST]Understood.</s>\",\n      \"user_start\": \"[INST]\",\n      \"user_end\": \"\",\n      \"assistant_start\": \"[/INST]\",\n      \"assistant_end\": \"</s>\"\n    },\n    \"mistral-v7-tekken\":  {\n      \"system_start\": \"[SYSTEM_PROMPT]\",\n      \"system_end\": \"[/SYSTEM_PROMPT]\",\n      \"user_start\": \"[INST]\",\n      \"user_end\": \"[/INST]\",\n      \"assistant_start\": \" \",\n      \"assistant_end\": \"</s>\"\n    },\n    \"gemma2\":{\n      \"system_start\": \"<start_of_turn>system\\n\",\n      \"system_end\": \"<end_of_turn>\\n\",\n      \"user_start\": \"<start_of_turn>user\\n\",\n      \"user_end\": \"<end_of_turn>\\n\",\n      \"assistant_start\": \"<start_of_turn>model\\n\",\n      \"assistant_end\": \"<end_of_turn>\\n\"\n    },\n    \"metharme\": {\n      \"system_start\": \"<|system|>\",\n      \"system_end\": \"\",\n      \"user_start\": \"<|user|>\",\n      \"user_end\": \"\",\n      \"assistant_start\": \"<|model>\",\n      \"assistant_end\": \"\"\n    },\n    \"harmony\": {\n        \"system_start\": \"<|start|>system<|message|>\",\n        \"system_end\": \"<|end|>\",\n        \"user_start\": \"<|start|>user<|message|>\",\n        \"user_end\": \"<|end|>\",\n        \"assistant_start\": \"<|start|>assistant<|channel|>final<|message|>\",\n        \"assistant_end\": \"<|end|>\"\n    },\n    \"glm4\": {\n        \"system_start\": \"[gMASK]<sop>\",\n        \"system_end\": \"\\n\",\n        \"user_start\": \"<|user|>\\n\",\n        \"user_end\": \"\",\n        \"assistant_start\": \"<|assistant|>\\n\",\n        \"assistant_end\": \"\"\n    },\n    \"personalityengine-custom\": {\n        \"system_start\": \"<|system|>\",\n        \"system_end\": \"<|endoftext|>\",\n        \"user_start\": \"<|user|>\",\n        \"user_end\": \"<|endoftext|>\",\n        \"assistant_start\": \"<|assistant|>\",\n        \"assistant_end\": \"<|endoftext|>\"\n    },\n}\n\nmodel_options = [\n    # additional models from recs\n    (\"PocketDoc: Dan's PersonalityEngine V1.3.0 24B Q6_K_L\", \"https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/blob/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf?download=true\", \"personalityengine-custom\"),\n    (\"Delta-Vector: MS3.2 Austral-Winton 24B Q6_K_L\", \"https://huggingface.co/bartowski/Delta-Vector_MS3.2-Austral-Winton-GGUF/resolve/main/Delta-Vector_MS3.2-Austral-Winton-Q6_K_L.gguf?download=true\", \"chatml\"),\n    (\"Gryphe: Codex 24B Q6_K\", \"https://huggingface.co/bartowski/Gryphe_Codex-24B-Small-3.2-GGUF/blob/main/Gryphe_Codex-24B-Small-3.2-Q6_K_L.gguf?download=true\", \"chatml\"),\n    (\"allura-org: GLM-4-Neon v2 32B Q6_K\", \"https://huggingface.co/mradermacher/GLM4-32B-Neon-v2-GGUF/resolve/main/GLM4-32B-Neon-v2.Q6_K.gguf?download=true\", \"glm4\"),\n    (\"allura-org: Gemma-3-Starshine 12B Q8\", \"https://huggingface.co/mradermacher/Gemma-3-Starshine-12B-GGUF/blob/main/Gemma-3-Starshine-12B.Q8_0.gguf?download=true\", \"gemma2\"),\n    (\"allura-org: Gemma-3-Glitter 12B Q8\", \"https://huggingface.co/mradermacher/Gemma-3-Glitter-12B-GGUF/resolve/main/Gemma-3-Glitter-12B.Q8_0.gguf?download=true\", \"gemma2\"),\n    (\"allura-org: Gemma-3-Glitter 27B Q4_K_M\", \"https://huggingface.co/mradermacher/Gemma-3-Glitter-27B-GGUF/resolve/main/Gemma-3-Glitter-27B.Q4_K_M.gguf?download=true\", \"gemma2\"),\n    (\"allura-org: RP-Ink 12B Q8\", \"https://huggingface.co/allura-org/MN-12b-RP-Ink-GGUF/resolve/main/MN-12b-RP-Ink-Q8_0.gguf?download=true\", \"mistral-v3-tekken\"),\n    (\"allura-org: RP-Ink 32B Q4_K_M\", \"https://huggingface.co/bartowski/Qwen2.5-32b-RP-Ink-GGUF/blob/main/Qwen2.5-32b-RP-Ink-Q4_K_M.gguf?download=true\", \"chatml\"),\n    (\"inflatebot: Mag-Mell-R1 12B Q8\", \"https://huggingface.co/inflatebot/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"TheDrummer: Cydonia v2.1 24B Q6_K_L\", \"https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v2.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v2.1-Q6_K_L.gguf?download=true\", \"mistral-v7-tekken\"),\n    (\"TheDrummer: Skyfall v4 31B Q4_K_M\", \"https://huggingface.co/TheDrummer/Skyfall-31B-v4-GGUF/blob/main/Skyfall-31B-v4j-Q4_K_M.gguf?download=true\", \"mistral-v7-tekken\"),\n    (\"EVA-UNIT-01: EVA-Qwen2.5 v0.2 32B Q4_K_M\", \"https://huggingface.co/mradermacher/EVA-Qwen2.5-32B-v0.2-GGUF/resolve/main/EVA-Qwen2.5-32B-v0.2.Q4_K_M.gguf?download=true\", \"chatml\"),\n    (\"ReadyArt: Amoral-Fallen-Omega-Gemma3 12B Q8\", \"https://huggingface.co/bartowski/ReadyArt_Amoral-Fallen-Omega-Gemma3-12B-GGUF/blob/main/ReadyArt_Amoral-Fallen-Omega-Gemma3-12B-Q8_0.gguf?download=true\", \"gemma2\"),\n    (\"ReadyArt: The-Omega-Directive-Qwen3 14B Q8\", \"https://huggingface.co/mradermacher/The-Omega-Directive-Qwen3-14B-v1.1-GGUF/resolve/main/The-Omega-Directive-Qwen3-14B-v1.1.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"ArliAI: QwQ RpR v4 32B Q4_K_M\", \"https://huggingface.co/ArliAI/QwQ-32B-ArliAI-RpR-v4-GGUF/resolve/main/QwQ-32B-ArliAI-RpR-v4-Q4_K_M.gguf?download=true\", \"chatml\"),\n    (\"Epiculous: Violet_Twilight-v0.2 12B Q8\", \"https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"trashpanda-org: Snowdrop v0 32B Q4_K_L\", \"https://huggingface.co/bartowski/trashpanda-org_QwQ-32B-Snowdrop-v0-GGUF/resolve/main/trashpanda-org_QwQ-32B-Snowdrop-v0-Q4_K_L.gguf?download=true\", \"chatml\"),\n    (\"LatitudeGames: Muse 12B Q8\", \"https://huggingface.co/LatitudeGames/Muse-12B-GGUF/resolve/main/Muse-12B-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"LatitudeGames: Wayfarer 2 12B Q8\", \"https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"LatitudeGames: Harbinger 24B Q6K\", \"https://huggingface.co/LatitudeGames/Harbinger-24B-GGUF/resolve/main/Harbinger-24B-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Tesslate: Synthia S1 27B Q4_K_M\", \"https://huggingface.co/mradermacher/Synthia-S1-27b-GGUF/resolve/main/Synthia-S1-27b.Q4_K_M.gguf?download=true\", \"gemma2\"),\n    (\"nbeerbower: mistral-nemo-gutenberg 12B Q8\", \"https://huggingface.co/mradermacher/mistral-nemo-gutenberg-12B-GGUF/resolve/main/mistral-nemo-gutenberg-12B.Q8_0.gguf?download=true\", \"mistral-v3-tekken\"),\n    (\"Mawdistical: Mawnia 12B Q8\", \"https://huggingface.co/Mawdistical/Mawnia-12B-GGUF/resolve/main/Mawdistical_Mawnia-12B-Q8_0.gguf?download=true\", \"gemma2\"),\n    (\"SicariusSicariiStuff: Impish Nemo 12B Q8\", \"https://huggingface.co/SicariusSicariiStuff/Impish_Nemo_12B_GGUF/blob/main/Impish_Nemo_12B-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"SicariusSicariiStuff: Impish Magic 24B Q6_K\", \"https://huggingface.co/SicariusSicariiStuff/Impish_Magic_24B_GGUF/resolve/main/SicariusSicariiStuff_Impish_Magic_24B-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Qwen: Qwen3 30B-A3B abliterated Q4_K_M\", \"https://huggingface.co/Sowkwndms/Qwen3-30B-A3B-abliterated-Q4_K_M-GGUF/resolve/main/qwen3-30b-a3b-abliterated-q4_k_m.gguf?download=true\", \"chatml\"),\n    (\"openai: gpt-oss 20B abliterated Q4_K_M\", \"https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M.gguf?download=true\", \"harmony\"),\n    (\"Llama-3-Stheno 8B Q8_0\", \"https://huggingface.co/QuantFactory/Llama-3.1-8B-Stheno-v3.4-GGUF/resolve/main/Llama-3.1-8B-Stheno-v3.4.Q8_0.gguf?download=true\", \"llama-3\"),\n    (\"Llama-3-Stheno-ULTRA 8B Q8_0\", \"https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13.gguf?download=true\", \"llama-3\"),\n    (\"Llama-3-SthenoMaidBlackroot 8B Q8_0\", \"https://huggingface.co/mradermacher/L3-SthenoMaidBlackroot-8B-V1-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1.Q8_0.gguf?download=true\", \"llama-3\"),\n    (\"Llama-3-Umbral-Mind 8B Q8_0\", \"https://huggingface.co/QuantFactory/L3-Umbral-Mind-RP-v3.0-8B-GGUF/resolve/main/L3-Umbral-Mind-RP-v3.0-8B.Q8_0.gguf?download=true\", \"llama-3\"),\n    (\"Llama-3-Hathor-Stable 8B Q8_0\", \"https://huggingface.co/mradermacher/Hathor_Stable-v0.2-L3-8B-GGUF/resolve/main/Hathor_Stable-v0.2-L3-8B.Q8_0.gguf?download=true\", \"llama-3\"),\n    (\"Llama-3-Lunaris 8B Q8_0\", \"https://huggingface.co/bartowski/L3-8B-Lunaris-v1-GGUF/resolve/main/L3-8B-Lunaris-v1-Q8_0_L.gguf?download=true\", \"llama-3\"),\n    (\"Llama-3.1-Dark-Planet-8-Orbs 8B Q8_0\", \"https://huggingface.co/DavidAU/L3-Dark-Planet-8B-V2-Eight-Orbs-Of-Power-GGUF/resolve/main/L3-Dark-Planet-8B-V2-EOOP-D_AU-Q8_0.gguf?download=true\", \"llama-3\"),\n    (\"Llama-3.1-DarkIdol 8B Q8_0\", \"https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf?download=true\", \"llama-3\"),\n    (\"Yodayo-Nephra 8B Q8_0\", \"https://huggingface.co/Marcus-Arcadius/nephra_v1.0-Q8_0-GGUF/resolve/main/nephra_v1.0-q8_0.gguf?download=true\", \"llama-3\"),\n    (\"Tarnished\", \"https://huggingface.co/mradermacher/tarnished-9b-GGUF/resolve/main/tarnished-9b.Q8_0.gguf?download=true\", \"gemma2\"),\n    (\"Gemma2-Ataraxy(9b)\", \"https://huggingface.co/bartowski/Gemma-2-Ataraxy-9B-GGUF/resolve/main/Gemma-2-Ataraxy-9B-Q8_0.gguf?download=true\", \"gemma2\"),\n    (\"Fimbulvetr2(11b)\", \"https://huggingface.co/Sao10K/Fimbulvetr-11B-v2-GGUF/resolve/main/Fimbulvetr-11B-v2-Test-14.q8_0.gguf?download=true\", \"alpaca\"),\n    (\"Kaiju(11b)\", \"https://huggingface.co/Himitsui/Kaiju-11B-GGUF/resolve/main/Kaiju-11B.q8_0.gguf?download=true\", \"alpaca\"),\n    (\"Mini-Magnum(12b)\", \"https://huggingface.co/mradermacher/mini-magnum-12b-v1.1-GGUF/resolve/main/mini-magnum-12b-v1.1.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Nemomix(12b)\", \"https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q8_0.gguf?download=true\", \"mistral-v7-tekken\"),\n    (\"Lyra(12b)\", \"https://huggingface.co/bartowski/MN-12B-Lyra-v4-GGUF/resolve/main/MN-12B-Lyra-v4-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Guns-and-roses(12b)\", \"https://huggingface.co/Reiterate3680/guns-and-roses-r1-GGUF/resolve/main/guns-and-roses-r1-Q8_0-imat.gguf?download=true\", \"chatml\"),\n    (\"Starcannon(12b)\", \"https://huggingface.co/bartowski/MN-12B-Starcannon-v3-GGUF/resolve/main/MN-12B-Starcannon-v3-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Rocinante(12b)\", \"https://huggingface.co/bartowski/Rocinante-12B-v1.1-GGUF/resolve/main/Rocinante-12B-v1.1-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Chronos Gold(12b)\", \"https://huggingface.co/bartowski/Chronos-Gold-12B-1.0-GGUF/resolve/main/Chronos-Gold-12B-1.0-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Mag-Mell(12b)\", \"https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Violet Twilight(12b)\", \"https://huggingface.co/mradermacher/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Halide(12b)\", \"https://huggingface.co/mradermacher/MN-Halide-12b-v1.0-i1-GGUF/resolve/main/MN-Halide-12b-v1.0.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Stellar Odyssey(12b)\", \"https://huggingface.co/bartowski/Stellar-Odyssey-12b-v0.0-GGUF/resolve/main/Stellar-Odyssey-12b-v0.0-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"DarkPlanet TITAN(12b)\", \"https://huggingface.co/DavidAU/MN-Dark-Planet-TITAN-12B-GGUF/resolve/main/MN-Dark-Planet-TITAN-12B-D_AU-Q8_0.gguf?download=true\", \"mistral-v7-tekken\"),\n    (\"Violet Lotus(12b)\", \"https://huggingface.co/QuantFactory/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Wayfarer(12b)\", \"https://huggingface.co/LatitudeGames/Wayfarer-2-12B-GGUF/resolve/main/Wayfarer-2-12B-Q8_0.gguf?download=true\", \"chatml\"),    \n    (\"Eidolon(14b)\", \"https://huggingface.co/mradermacher/Eidolon-v3-14B-GGUF/resolve/main/Eidolon-v3-14B.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Sugarquill(14b)\", \"https://huggingface.co/bartowski/TQ2.5-14B-Sugarquill-v1-GGUF/resolve/main/TQ2.5-14B-Sugarquill-v1-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Freya(14b)\", \"https://huggingface.co/bartowski/14B-Qwen2.5-Freya-x1-GGUF/resolve/main/14B-Qwen2.5-Freya-x1-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Kunou(32b)\", \"https://huggingface.co/bartowski/32B-Qwen2.5-Kunou-v1-GGUF/resolve/main/32B-Qwen2.5-Kunou-v1-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Cydonia(22b)\", \"https://huggingface.co/MarsupialAI/Cydonia-22B-v1_iMat_GGUF/resolve/main/Cydonia-22B-v1_iQ4xs.gguf?download=true\", \"chatml\"),\n    (\"Magnum(22b)\", \"https://huggingface.co/Quant-Cartel/SorcererLM-22B-iMat-GGUF/resolve/main/SorcererLM-22B-iMat-Q6_K.gguf?download=true\", \"mistral-v3-tekken\"),\n    (\"Sorcerer(22b)\", \"https://huggingface.co/Quant-Cartel/SorcererLM-22B-iMat-GGUF/resolve/main/SorcererLM-22B-iMat-Q6_K.gguf?download=true\", \"vicuna\")\n]\n\n_dropdown_options = [(label, (url, instruct)) for (label, url, instruct) in model_options]\n\nmodel_dropdown = widgets.Dropdown(\n    options=_dropdown_options,\n    value=_dropdown_options[0][1],\n    description='Model:',\n    layout=widgets.Layout(width='100%')\n)\n\ncustom_url = widgets.Text(\n    value='',\n    placeholder='Or paste a custom GGUF URL here...',\n    description='Custom URL:',\n    layout=widgets.Layout(width='100%')\n)\n\nuse_custom = widgets.Checkbox(value=False, description='Use custom URL')\n\ncontext_size_input = widgets.IntText(\n    value=16384,\n    description='Context:',\n    layout=widgets.Layout(width='200px')\n)\n\nmax_tokens_input = widgets.IntText(\n    value=2048,\n    description='Max tokens:',\n    layout=widgets.Layout(width='200px')\n)\n\ninstruct_preset_dropdown = widgets.Dropdown(\n    options=list(premade_instruct.keys()),\n    value=\"chatml\",\n    description='Instruct Preset:',\n    layout=widgets.Layout(width='300px')\n)\n\nlayers_input = widgets.IntText(value=99, description='Layers:')\nkvcache_input = widgets.Dropdown(options=[\"0\",\"1\",\"2\"], value=\"0\", description='KvCache:')\nblayer_input = widgets.IntText(value=99, description='blayer:')\n\ndef get_selected_url():\n    return custom_url.value.strip() if use_custom.value and custom_url.value.strip() else model_dropdown.value[0]\n\ndef get_selected_instruct():\n    return instruct_preset_dropdown.value if use_custom.value else model_dropdown.value[1]\n\ndef _on_model_change(change):\n    url, instruct = change['new']\n    instruct_preset_dropdown.value = instruct\n\nmodel_dropdown.observe(_on_model_change, names='value')\n\n# Display UI (models / core settings)\nui_models = widgets.VBox([\n    model_dropdown,\n    widgets.HBox([use_custom, custom_url]),\n    widgets.HBox([context_size_input, max_tokens_input]),\n    widgets.HTML(value=\"<hr>\"),\n    instruct_preset_dropdown,\n    widgets.HTML(value=\"<hr>\"),\n    widgets.HTML(value=\"<span>Advanced settings: Do not tweak unless you know what you are doing.</span>\"),\n    widgets.HBox([layers_input, kvcache_input, blayer_input])\n])\n\ndisplay(ui_models)\n\n_SELECTED_URL_VALUE = get_selected_url()\n_CONTEXT_SIZE_VALUE = int(context_size_input.value)\n_MAX_TOKENS_VALUE = int(max_tokens_input.value)\n_INSTRUCT_PRESET_VALUE = get_selected_instruct()\n_LAYERS_VALUE = int(layers_input.value)\n_KVCACHE_VALUE = kvcache_input.value\n_BLAYER_VALUE = int(blayer_input.value)\n\ndef _on_any_change(change):\n    global _SELECTED_URL_VALUE, _CONTEXT_SIZE_VALUE, _MAX_TOKENS_VALUE\n    global _INSTRUCT_PRESET_VALUE\n    global _LAYERS_VALUE, _KVCACHE_VALUE, _BLAYER_VALUE\n    _SELECTED_URL_VALUE = get_selected_url()\n    _CONTEXT_SIZE_VALUE = int(context_size_input.value)\n    _MAX_TOKENS_VALUE = int(max_tokens_input.value)\n    _INSTRUCT_PRESET_VALUE = get_selected_instruct()\n    _LAYERS_VALUE = int(layers_input.value)\n    _KVCACHE_VALUE = kvcache_input.value\n    _BLAYER_VALUE = int(blayer_input.value)\n\nfor w in [model_dropdown, custom_url, use_custom,\n          context_size_input, max_tokens_input,\n          instruct_preset_dropdown,\n          layers_input, kvcache_input, blayer_input]:\n    w.observe(_on_any_change, names='value')\n\n\n# -------------------------------\n# Proxy + Sampler settings UI\n# -------------------------------\n\n# Managed internals (hidden from users)\n_PROXY_PORT = 5002\n_KOBOLD_BASE = 'http://127.0.0.1:5001'\n\n# Defaults aligned with jai-proxy-suite\n_instruct_from_notebook = globals().get('_INSTRUCT_PRESET_VALUE', 'chatml')\nweb_param = {\n    \"instruct\": _instruct_from_notebook,\n    \"top_p\": 0.92,\n    \"min_p\": 0.12,\n    \"top_k\": -1,\n    \"repetition_penalty\": 1.05,\n    \"frequency_penalty\": 0,\n    \"presence_penalty\": 0.26,\n    \"banned_strings\": [],\n    \"dry_enabled\": False,\n    \"dry_multiplier\": 1.75,\n    \"dry_base\": 1.1,\n    \"dry_allowed_length\": 3,\n    \"dry_range\": 1024,\n}\n\n# Sampler controls only\n_top = widgets.HBox([\n    widgets.FloatText(value=0.92, description='top_p:'),\n    widgets.FloatText(value=0.12, description='min_p:'),\n    widgets.IntText(value=-1, description='top_k:'),\n])\n_rep = widgets.HBox([\n    widgets.FloatText(value=1.05, description='rep_pen:'),\n    widgets.FloatText(value=0.0, description='freq_pen:'),\n    widgets.FloatText(value=0.26, description='pres_pen:'),\n])\n\nbanned_strings_input = widgets.Textarea(\n    value=\"\",\n    description='banned_strings:',\n    layout=widgets.Layout(width='100%', height='80px')\n)\n\ndry_enabled_chk = widgets.Checkbox(value=False, description='DRY enabled')\ndry_mult_input = widgets.FloatText(value=1.75, description='dry_mult:')\ndry_base_input = widgets.FloatText(value=1.1, description='dry_base:')\ndry_len_input = widgets.IntText(value=3, description='dry_len:')\ndry_range_input = widgets.IntText(value=1024, description='dry_range:')\n\ndef _sync_from_widgets(_=None):\n    # Always refresh instruct from the model section (first part of this cell)\n    web_param[\"instruct\"] = globals().get('_INSTRUCT_PRESET_VALUE', 'chatml')\n\n    # Map sampler inputs from UI containers\n    web_param[\"top_p\"] = float(_top.children[0].value)\n    web_param[\"min_p\"] = float(_top.children[1].value)\n    web_param[\"top_k\"] = int(_top.children[2].value)\n\n    web_param[\"repetition_penalty\"] = float(_rep.children[0].value)\n    web_param[\"frequency_penalty\"] = float(_rep.children[1].value)\n    web_param[\"presence_penalty\"] = float(_rep.children[2].value)\n\n    banned_raw = banned_strings_input.value.strip()\n    web_param[\"banned_strings\"] = [s.strip() for s in banned_raw.split(',')] if banned_raw else []\n\n    web_param[\"dry_enabled\"] = bool(dry_enabled_chk.value)\n    web_param[\"dry_multiplier\"] = float(dry_mult_input.value)\n    web_param[\"dry_base\"] = float(dry_base_input.value)\n    web_param[\"dry_allowed_length\"] = int(dry_len_input.value)\n    web_param[\"dry_range\"] = int(dry_range_input.value)\n\n# Attach observers to every relevant widget\nfor w in [\n    _top.children[0], _top.children[1], _top.children[2],\n    _rep.children[0], _rep.children[1], _rep.children[2],\n    banned_strings_input,\n    dry_enabled_chk, dry_mult_input, dry_base_input, dry_len_input, dry_range_input\n]:\n    w.observe(_sync_from_widgets, names='value')\n\n# Initial sync so web_param matches defaults immediately\n_sync_from_widgets()\n\nui_sampler = widgets.VBox([\n    widgets.HTML(value=\"<b>Sampler params</b>\"),\n    _top,\n    _rep,\n    banned_strings_input,\n    widgets.HTML(value=\"<hr><b>DRY Sampling</b>\"),\n    widgets.HBox([dry_enabled_chk, dry_mult_input, dry_base_input, dry_len_input, dry_range_input])\n])\n\ndisplay(ui_sampler)\n","metadata":{"execution":{"iopub.execute_input":"2025-09-23T17:14:52.779936Z","iopub.status.busy":"2025-09-23T17:14:52.779737Z","iopub.status.idle":"2025-09-23T17:14:52.993263Z","shell.execute_reply":"2025-09-23T17:14:52.99268Z","shell.execute_reply.started":"2025-09-23T17:14:52.779917Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Reverse proxy setup\n\n#### How to tell when this cell is done:\n\n`Proxy running on port 5002, forwarding to http://127.0.0.1:5001` should appear in the cell's output.","metadata":{}},{"cell_type":"code","source":"# Reverse proxy for Kobold\n!pip -q install flask flask_cors requests\nfrom flask import Flask, request, jsonify, Response, stream_with_context\nfrom flask_cors import CORS\nimport requests as _rq\nimport json as _json\nimport time as _time\n\napp = Flask(__name__)\nCORS(app)\n\n# defaults if UI not run\ntry:\n    _ = _PROXY_PORT\nexcept NameError:\n    _PROXY_PORT = 5002\ntry:\n    _ = _KOBOLD_BASE\nexcept NameError:\n    _KOBOLD_BASE = 'http://127.0.0.1:5001'\ntry:\n    _ = web_param\nexcept NameError:\n    web_param = {\n        \"instruct\": \"chatml\",\n        \"top_p\": 0.92,\n        \"min_p\": 0.12,\n        \"top_k\": -1,\n        \"repetition_penalty\": 1.05,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0.26,\n        \"banned_strings\": [],\n        \"dry_enabled\": False,\n        \"dry_multiplier\": 1.75,\n        \"dry_base\": 1.1,\n        \"dry_allowed_length\": 3,\n        \"dry_range\": 1024,\n    }\n\n# prompt adapter like jai-proxy-suite\ntry:\n    _ = premade_instruct\nexcept NameError:\n    premade_instruct = {\n        \"chatml\": {\n            \"system_start\": \"<|im_start|>system\",\n            \"system_end\": \"<|im_end|>\",\n            \"user_start\": \"<|im_start|>user\",\n            \"user_end\": \"<|im_end|>\",\n            \"assistant_start\": \"<|im_start|>assistant\",\n            \"assistant_end\": \"<|im_end|>\",\n        }\n    }\n\n\ndef message_instructor(messages_list, preset=None):\n    adapter_key = preset or web_param.get('instruct', 'chatml')\n    adapter = premade_instruct.get(adapter_key, premade_instruct['chatml'])\n    ss = adapter.get('system_start', '')\n    se = adapter.get('system_end', '')\n    us = adapter.get('user_start', '')\n    ue = adapter.get('user_end', '')\n    as_ = adapter.get('assistant_start', '')\n    ae = adapter.get('assistant_end', '')\n    out = []\n    for m in messages_list:\n        if m['role'] == 'system':\n            out.append(ss + m['content'] + se)\n        elif m['role'] == 'user':\n            out.append(us + m['content'] + ue)\n        elif m['role'] == 'assistant':\n            out.append(as_ + m['content'] + ae)\n        elif m['role'] == 'tool':\n            out.append(m['content'])\n    out.append(as_)\n    return ''.join(out)\n\n\ndef build_kobold_payload(src):\n    # Keep instruct preset in sync with the model selection UI\n    try:\n        web_param['instruct'] = globals().get('_INSTRUCT_PRESET_VALUE', web_param.get('instruct', 'chatml'))\n    except Exception:\n        pass\n\n    # Take OpenAI Chat Completions body and map to Kobold-compatible params\n    body = {\n        'model': src.get('model', ''),\n        'temperature': src.get('temperature', 0.9),\n        'max_tokens': src.get('max_tokens', 2048),\n        'min_p': web_param['min_p'],\n        'top_p': web_param['top_p'],\n        'top_k': web_param['top_k'],\n        'repetition_penalty': web_param['repetition_penalty'],\n        'presence_penalty': web_param['presence_penalty'],\n        'frequency_penalty': web_param['frequency_penalty'],\n        'banned_strings': web_param['banned_strings'],\n        'n': 1,\n        'best_of': 1,\n        'skip_special_tokens': True,\n        'sampler_order': [6,0,1,3,4,2,5]\n    }\n    # DRY sampling pass-through\n    if web_param.get('dry_enabled'):\n        body.update({\n            'dry_allowed_length': web_param['dry_allowed_length'],\n            'dry_base': web_param['dry_base'],\n            'dry_multiplier': web_param['dry_multiplier'],\n            'dry_penalty_last_n': web_param['dry_range'],\n            'dry_sequence_breakers': web_param.get('dry_sequence_breakers', None)\n        })\n    # Adapt messages to single prompt if backend expects that; Kobold v1 chat supports messages\n    if 'messages' in src:\n        body['messages'] = src['messages']\n    if src.get('messages') and web_param.get('instruct'):\n        # also build an instruct prompt for adapters that need it (some kobold variants)\n        body['prompt'] = message_instructor(src['messages'])\n    # stream flag\n    body['stream'] = bool(src.get('stream', False))\n    return body\n\n\ndef forward_stream(endpoint_url, config):\n    def gen():\n        try:\n            with _rq.post(endpoint_url, json=config, stream=True) as r:\n                r.raise_for_status()\n                for line in r.iter_lines():\n                    if not line:\n                        continue\n                    text = line.decode('utf-8')\n                    yield f\"{text}\\n\\n\"\n                    _time.sleep(0.02)\n        except Exception as e:\n            err = {\"error\": str(e)}\n            yield f\"data: {_json.dumps(err)}\\n\\n\"\n    return Response(stream_with_context(gen()), content_type='text/event-stream')\n\n\ndef forward_normal(endpoint_url, config):\n    try:\n        r = _rq.post(endpoint_url, json=config)\n        r.raise_for_status()\n        data = r.json()\n        # auto-trim like suite (simple sentence boundary trim)\n        try:\n            txt = data.get('choices', [{}])[0].get('message', {}).get('content')\n            if isinstance(txt, str):\n                data['choices'][0]['message']['content'] = txt.rstrip()\n        except Exception:\n            pass\n        return jsonify(data)\n    except Exception as e:\n        return Response(_json.dumps({\"error\": str(e)}), status=500, content_type='application/json')\n\n\n@app.route('/v1/chat/completions', methods=['POST'])\ndef proxy_chat():\n    if not request.json:\n        return jsonify(error=True), 400\n    endpoint = f\"{_KOBOLD_BASE}/v1/chat/completions\"\n    payload = build_kobold_payload(request.json)\n    if payload.get('stream'):\n        return forward_stream(endpoint, payload)\n    else:\n        return forward_normal(endpoint, payload)\n\n\nimport threading as _th\n\ndef _run_proxy():\n    app.run(host='0.0.0.0', port=_PROXY_PORT, use_reloader=False)\n\ntry:\n    PROXY_THREAD\n    # If already defined, do nothing or restart logic could be added here\nexcept NameError:\n    PROXY_THREAD = _th.Thread(target=_run_proxy, daemon=True)\n    PROXY_THREAD.start()\nprint(f\"Proxy running on port {_PROXY_PORT}, forwarding to {_KOBOLD_BASE}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-09-23T17:17:59.414214Z","iopub.status.busy":"2025-09-23T17:17:59.413723Z","iopub.status.idle":"2025-09-23T17:18:04.285885Z","shell.execute_reply":"2025-09-23T17:18:04.284127Z","shell.execute_reply.started":"2025-09-23T17:17:59.414194Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Download model\n\n#### How to tell when this cell is done:\n\nThe name of the model you selected should appear below, as well as a full download progress bar.","metadata":{}},{"cell_type":"code","source":"# Download selected model\nimport os\n!pip install hf_transfer\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\ntry:\n    URL = _SELECTED_URL_VALUE if '_SELECTED_URL_VALUE' in globals() and _SELECTED_URL_VALUE else URL\nexcept NameError:\n    URL = \"https://huggingface.co/bartowski/trashpanda-org_QwQ-32B-Snowdrop-v0-GGUF/resolve/main/trashpanda-org_QwQ-32B-Snowdrop-v0-Q4_K_L.gguf\"\n\nfrom urllib.parse import urlparse, parse_qs\n\ndef extract_parts(url):\n    parsed_url = urlparse(url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    query = parse_qs(parsed_url.query)\n    \n    model_name = path_parts[0]+\"/\"+path_parts[1]\n    version = path_parts[3]\n    file_name = path_parts[4]\n    \n    return model_name, version, file_name\n\nREPO_ID, REVISION, FILE = extract_parts(URL)\n\nDIR = REPO_ID.replace(\"/\", \"_\")\n\nif REVISION != \"\" and REVISION != \"main\":\n    DIR = f\"{DIR}_{REVISION}\"\n\nprint(\"Model Name: \" + DIR)\nprint(\"Version: \" + REVISION)\nprint(\"File: \" + FILE)\n\nfrom huggingface_hub import hf_hub_download\n\nFULLPATH = f\"/kaggle/models/{DIR}\"\n\nif REVISION != \"\":\n    hf_hub_download(repo_id=REPO_ID, filename=FILE, local_dir_use_symlinks=False, revision=REVISION, local_dir=FULLPATH)\n    \nif REVISION == \"\":\n    hf_hub_download(repo_id=REPO_ID, filename=FILE, local_dir_use_symlinks=False, local_dir=FULLPATH)","metadata":{"execution":{"iopub.execute_input":"2025-09-23T17:18:10.511796Z","iopub.status.busy":"2025-09-23T17:18:10.51094Z","iopub.status.idle":"2025-09-23T17:20:42.236844Z","shell.execute_reply":"2025-09-23T17:20:42.235924Z","shell.execute_reply.started":"2025-09-23T17:18:10.511765Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Expose endpoint via ngrok\n\n#### How to tell when this cell is done:\n\nA button containing `Copy API Link` should appear in the cell output.","metadata":{}},{"cell_type":"code","source":"# ngrok setup\n!pip3 install pyngrok\nfrom kaggle_secrets import UserSecretsClient\nfrom pyngrok import ngrok\nsecret_label = \"ngrok-auth\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!ngrok config add-authtoken {secret_value}\ntunnel = ngrok.connect(5002)\nprint(\"Your remote link is: \" + tunnel.public_url)\nfrom IPython.display import HTML\nhtml = f'''\n    <div>\n    <h4>\n    <a href=\"{tunnel.public_url}/v1\" id=\"api\">{tunnel.public_url}/v1</a>\n    <button onclick=\"copyToClipboard('api')\">Copy API Link</button>\n    </h4>\n    </div>\n    \n    <script>\n    function copyToClipboard(copy) {{\n        var textToCopy = document.getElementById(copy).innerText;\n        var tempTextarea = document.createElement(\"textarea\");\n        tempTextarea.value = textToCopy;\n        document.body.appendChild(tempTextarea);\n        tempTextarea.select();\n        tempTextarea.setSelectionRange(0, 99999);\n        document.execCommand(\"copy\");\n        document.body.removeChild(tempTextarea);\n        alert(\"Copied the text: \" + textToCopy);\n    }}\n    </script>\n'''\n\ndisplay(HTML(html))","metadata":{"execution":{"iopub.execute_input":"2025-09-23T17:23:59.950185Z","iopub.status.busy":"2025-09-23T17:23:59.949481Z","iopub.status.idle":"2025-09-23T17:24:11.020768Z","shell.execute_reply":"2025-09-23T17:24:11.020156Z","shell.execute_reply.started":"2025-09-23T17:23:59.95016Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Launch Kobold\n\n#### How to tell when this cell is done:\n\n`Please connect to custom endpoint at http://localhost:5001` should appear in the cell output.","metadata":{}},{"cell_type":"code","source":"# Start Kobold\nMODEL = FULLPATH + \"/\" + FILE\nprint(\"Model to load: \" + MODEL)\n\nimport json as _json\n_preset = _INSTRUCT_PRESET_VALUE if '_INSTRUCT_PRESET_VALUE' in globals() else 'chatml'\n_tpl = premade_instruct.get(_preset, premade_instruct['chatml'])\nwith open('instruct.json', 'w') as f:\n    f.write(_json.dumps(_tpl, separators=(\",\", \":\")))\n\n_layers = _LAYERS_VALUE if '_LAYERS_VALUE' in globals() else 999\n\n!./koboldcpp {MODEL} --contextsize {_CONTEXT_SIZE_VALUE if '_CONTEXT_SIZE_VALUE' in globals() else 24000} --usecublas 0 1 normal mmq rowsplit --blasbatchsize 512 --flashattention --foreground --gpulayers {_layers} --quiet --threads 999 --blasthreads 999 --nommap --tensor_split 1 1 --skiplauncher --defaultgenamt={_MAX_TOKENS_VALUE if '_MAX_TOKENS_VALUE' in globals() else 2048} --chatcompletionsadapter instruct.json","metadata":{"execution":{"iopub.execute_input":"2025-09-23T17:24:20.43166Z","iopub.status.busy":"2025-09-23T17:24:20.430891Z"},"trusted":true},"outputs":[],"execution_count":null}]}