{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/severiandev/koboldcpp-notebook?scriptVersionId=263392674\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# KoboldCPP on Kaggle\n\nHost models up to 32B with this notebook. A slightly more organized version of [Divine's notebook here](https://www.kaggle.com/code/divinesinner/koboldcpp-guide-in-comment/notebook).\n\n### Relevant links\n- [Divine's guide](https://www.kaggle.com/code/divinesinner/koboldcpp-guide-in-comment/comments#3102042)\n- [Hibiki's model recommendations on the unofficial Colab fork](https://colab.research.google.com/drive/1l_wRGeD-LnRl3VtZHDc7epW_XW0nJvew#scrollTo=pf4AQOYgTB2d)\n- [nyxkrage's VRAM Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)\n- [Myscell's local model recommendations](https://rentry.org/anathem)\n- [Pepper's local model reviews](https://www.notion.so/playwithpepper/1f392d900248803f86c2c51c73f92a0b?v=1f392d90024880d59e33000cc1b15175)\n- [Naen's Kaggle guide, if not a bit outdated](https://rentry.org/GodsGreatestKaggles)\n- ~~[trashpanda-org on HF, just because I can](https://huggingface.co/trashpanda-org)~~\n\n### Getting started\n\nMake sure you've done the following (refer to Divine's guide above if unsure how):\n\n1. Sign up for a **Kaggle** account with a **verified phone number**. You might be asked to do facial verification via Persona.\n2. Sign up for an **ngrok** account and acquire an **authtoken** [(signup/login here)](https://ngrok.com/)\n3. In the **`Settings`** menu above -> **`Accelerator`**, select **`GPU T4x2`**\n\n### Selecting a model to run\n\nTo be specific, you need a model quant URL for Kobold to download and set up with.\nToo long and disruptive for a guide to be here, so just check out [this model quant selection guide](https://rentry.org/severian) for more details, including some screenshots.\n\n### Using this notebook\n\n> Oh, to my knowledge the runtime kills itself after 40 minutes, the audio curbs that. I believe it was Divine's idea. Although I mostly uses kaggle on my phone, for PC you might need a script that automatically clicks the screen in a set interval.\n\nFrom Sam. It's recommended to use the audio file below to keep Kaggle from killing the runtime after 49 minutes. Run the cell first, before playing the audio file (note how these are two different steps.)\n\nAlso, if you have the max tokens setting as 0 in any frontend that allows such a thing, default max token is set to 2048 below.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"%%html\n<h5>Press play on the music player to keep the tab alive (Uses only 13MB of data)</h5>\n<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2025-09-17T09:06:17.265168Z","iopub.status.busy":"2025-09-17T09:06:17.264782Z","iopub.status.idle":"2025-09-17T09:06:17.271257Z","shell.execute_reply":"2025-09-17T09:06:17.270426Z","shell.execute_reply.started":"2025-09-17T09:06:17.265145Z"},"trusted":true},"outputs":[{"data":{"text/html":["<h5>Press play on the music player to keep the tab alive, then start block below (Uses only 13MB of data)</h5>\n","<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"execution_count":null},{"cell_type":"markdown","source":"<h3>Download Kobold</h3>","metadata":{}},{"cell_type":"code","source":"!curl -fLo koboldcpp https://github.com/LostRuins/koboldcpp/releases/download/v1.98.1/koboldcpp-linux-x64 && chmod +x koboldcpp","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T17:32:34.087906Z","iopub.execute_input":"2025-09-22T17:32:34.088453Z","iopub.status.idle":"2025-09-22T17:32:37.437047Z","shell.execute_reply.started":"2025-09-22T17:32:34.088428Z","shell.execute_reply":"2025-09-22T17:32:37.436342Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  561M  100  561M    0     0   180M      0  0:00:03  0:00:03 --:--:--  242M\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Select model, context and max tokens\n\nAfter running this cell, make changes to the settings below (select model, check context and max token) before moving on to the cells after this one.","metadata":{}},{"cell_type":"code","source":"# Model list, settings such as context, max tokens, instruct preset, and advanced settings\nfrom IPython.display import display\nimport ipywidgets as widgets\n\npremade_instruct = {\n    \"alpaca\": {\n        \"system_start\": \"\\n### Input: \",\n        \"system_end\": \"\",\n        \"user_start\": \"\\n### Instruction: \",\n        \"user_end\": \"\",\n        \"assistant_start\": \"\\n### Response: \",\n        \"assistant_end\": \"\",\n    },\n    \"vicuna\": {\n        \"system_start\": \"\\nSYSTEM: \",\n        \"system_end\": \"\",\n        \"user_start\": \"\\nUSER: \",\n        \"user_end\": \"\",\n        \"assistant_start\": \"\\nASSISTANT: \",\n        \"assistant_end\": \"\",\n    },\n    \"llama-3\": {\n        \"system_start\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n        \"system_end\": \"<|eot_id|>\",\n        \"user_start\": \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n        \"user_end\": \"<|eot_id|>\",\n        \"assistant_start\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n        \"assistant_end\": \"<|eot_id|>\",\n    },\n    \"chatml\": {\n        \"system_start\": \"<|im_start|>system\",\n        \"system_end\": \"<|im_end|>\",\n        \"user_start\": \"<|im_start|>user\",\n        \"user_end\": \"<|im_end|>\",\n        \"assistant_start\": \"<|im_start|>assistant\",\n        \"assistant_end\": \"<|im_end|>\",\n    },\n    \"command-r\": {\n        \"system_start\": \"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\",\n        \"system_end\": \"<|END_OF_TURN_TOKEN|>\",\n        \"user_start\": \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\",\n        \"user_end\": \"<|END_OF_TURN_TOKEN|>\",\n        \"assistant_start\": \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\",\n        \"assistant_end\": \"<|END_OF_TURN_TOKEN|>\",\n    },\n    \"mistral\":  {\n      \"system_start\": \"\",\n      \"system_end\": \"\",\n      \"user_start\": \"[INST] \",\n      \"user_end\": \"\",\n      \"assistant_start\": \" [/INST]\",\n      \"assistant_end\": \"</s> \"\n    },\n    \"mistral-v7-tekken\":  {\n      \"system_start\": \"[SYSTEM_PROMPT]\",\n      \"system_end\": \"[/SYSTEM_PROMPT]\",\n      \"user_start\": \"[INST]\",\n      \"user_end\": \"[/INST]\",\n      \"assistant_start\": \" \",\n      \"assistant_end\": \"</s>\"\n    },\n    \"gemma2\":{\n      \"system_start\": \"<start_of_turn>system\\n\",\n      \"system_end\": \"<end_of_turn>\\n\",\n      \"user_start\": \"<start_of_turn>user\\n\",\n      \"user_end\": \"<end_of_turn>\\n\",\n      \"assistant_start\": \"<start_of_turn>model\\n\",\n      \"assistant_end\": \"<end_of_turn>\\n\"\n    },\n    \"metharme\": {\n      \"system_start\": \"<|system|>\",\n      \"system_end\": \"\",\n      \"user_start\": \"<|user|>\",\n      \"user_end\": \"\",\n      \"assistant_start\": \"<|model>\",\n      \"assistant_end\": \"\"\n    },\n    \"harmony\": {\n        \"system_start\": \"<|start|>system<|message|>\",\n        \"system_end\": \"<|end|>\",\n        \"user_start\": \"<|start|>user<|message|>\",\n        \"user_end\": \"<|end|>\",\n        \"assistant_start\": \"<|start|>assistant<|channel|>final<|message|>\",\n        \"assistant_end\": \"<|end|>\"\n    },\n    \"personalityengine-custom\": {\n        \"system_start\": \"<|system|>\",\n        \"system_end\": \"<|endoftext|>\",\n        \"user_start\": \"<|user|>\",\n        \"user_end\": \"<|endoftext|>\",\n        \"assistant_start\": \"<|assistant|>\",\n        \"assistant_end\": \"<|endoftext|>\"\n    },\n}\n\nmodel_options = [\n    (\"[CATEGORY] 09/23 additions\", \"https://huggingface.co/bartowski/trashpanda-org_QwQ-32B-Snowdrop-v0-GGUF/resolve/main/trashpanda-org_QwQ-32B-Snowdrop-v0-Q4_K_L.gguf\", \"chatml\"),\n    # additional models from recs\n    (\"PocketDoc: Dan's PersonalityEngine V1.3.0 24B Q6_K_L\", \"https://huggingface.co/bartowski/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-GGUF/blob/main/PocketDoc_Dans-PersonalityEngine-V1.3.0-24b-Q6_K_L.gguf?download=true\", \"personalityengine-custom\"),\n    (\"allura-org: Gemma-3-Glitter 27B Q4_K_M\", \"https://huggingface.co/mradermacher/Gemma-3-Glitter-27B-GGUF/resolve/main/Gemma-3-Glitter-27B.Q4_K_M.gguf?download=true\", \"gemma2\"),\n    (\"Qwen: Qwen3 30B-A3B abliterated Q4_K_M\", \"https://huggingface.co/Sowkwndms/Qwen3-30B-A3B-abliterated-Q4_K_M-GGUF/resolve/main/qwen3-30b-a3b-abliterated-q4_k_m.gguf?download=true\", \"chatml\"),\n    (\"openai: gpt-oss 20B abliterated Q4_K_M\", \"https://huggingface.co/bartowski/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-GGUF/resolve/main/huihui-ai_Huihui-gpt-oss-20b-BF16-abliterated-Q4_K_M.gguf?download=true\", \"harmony\"),\n    # notebook originals\n    (\"Cydonia v2.1 24B Q6_K_L\", \"https://huggingface.co/bartowski/TheDrummer_Cydonia-24B-v2.1-GGUF/resolve/main/TheDrummer_Cydonia-24B-v2.1-Q6_K_L.gguf\", \"mistral-v7-tekken\"),\n    (\"trashpanda-org: Snowdrop v0 32B Q4_K_L\", \"https://huggingface.co/bartowski/trashpanda-org_QwQ-32B-Snowdrop-v0-GGUF/resolve/main/trashpanda-org_QwQ-32B-Snowdrop-v0-Q4_K_L.gguf\", \"chatml\"),\n    (\"trashpanda-org: Snowdrop v0 32B Q4_K_L\", \"https://huggingface.co/bartowski/trashpanda-org_QwQ-32B-Snowdrop-v0-GGUF/resolve/main/trashpanda-org_QwQ-32B-Snowdrop-v0-Q4_K_L.gguf\", \"chatml\"),\n    \n    \n    \n    \n    (\"[CATEGORY] OLD COLAB NOTEBOOK MODELS\", \"https://huggingface.co/bartowski/trashpanda-org_QwQ-32B-Snowdrop-v0-GGUF/resolve/main/trashpanda-org_QwQ-32B-Snowdrop-v0-Q4_K_L.gguf\", \"chatml\"),\n    # old hibiki colab models - need to update quant\n    (\"Kunoichi 7B Q8_0\", \"https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q8_0-imatrix.gguf?download=true\", \"chatml\"),\n    (\"WizardIceLemonTeaRP 32k Q8_0\", \"https://huggingface.co/mradermacher/WizardIceLemonTeaRP-32k-GGUF/resolve/main/WizardIceLemonTeaRP-32k.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"WizardLaker 7B Q8_0\", \"https://huggingface.co/mradermacher/WizardLaker-7B-GGUF/resolve/main/WizardLaker-7B.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"StunnaMaid 7B v0.2 Q8_0\", \"https://huggingface.co/Lewdiculous/Nyanade_Stunna-Maid-7B-v0.2-GGUF-IQ-Imatrix/resolve/main/Nyanade_Stunna-Maid-7B-v0.2-Q8_0-imat.gguf?download=true\", \"chatml\"),\n    (\"LemonKunoichiWizard Q8_0\", \"https://huggingface.co/mradermacher/LemonKunoichiWizardV3-GGUF/resolve/main/LemonKunoichiWizardV3.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Halu-Blackroot 8B Q8_0\", \"https://huggingface.co/mradermacher/Halu-8B-Llama3-Blackroot-GGUF/resolve/main/Halu-8B-Llama3-Blackroot.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Lumimaid 8B Q8_0\", \"https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix/resolve/main/Llama-3-Lumimaid-8B-v0.1-OAS-Q8_0-imat.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Daybreak-Lumimaid 8B Q8_0\", \"https://huggingface.co/mradermacher/llama3-daybreak-lumimaid0.1-8b-hf-GGUF/resolve/main/llama3-daybreak-lumimaid0.1-8b-hf.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Stheno 8B Q8_0\", \"https://huggingface.co/QuantFactory/Llama-3.1-8B-Stheno-v3.4-GGUF/resolve/main/Llama-3.1-8B-Stheno-v3.4.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Stheno-ULTRA 8B Q8_0\", \"https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-SthenoMaidBlackroot 8B Q8_0\", \"https://huggingface.co/mradermacher/L3-SthenoMaidBlackroot-8B-V1-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Umbral-Mind 8B Q8_0\", \"https://huggingface.co/QuantFactory/L3-Umbral-Mind-RP-v3.0-8B-GGUF/resolve/main/L3-Umbral-Mind-RP-v3.0-8B.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Hathor-Stable 8B Q8_0\", \"https://huggingface.co/mradermacher/Hathor_Stable-v0.2-L3-8B-GGUF/resolve/main/Hathor_Stable-v0.2-L3-8B.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Chara-Alpha 8B Q8_0\", \"https://huggingface.co/mradermacher/L3-8B-Chara-v1-Alpha-GGUF/resolve/main/L3-8B-Chara-v1-Alpha.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Hathor-Sofit 8B Q8_0\", \"https://huggingface.co/mradermacher/Hathor_Sofit-L3-8B-v1-GGUF/resolve/main/Hathor_Sofit-L3-8B-v1.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3-Lunaris 8B Q8_0\", \"https://huggingface.co/bartowski/L3-8B-Lunaris-v1-GGUF/resolve/main/L3-8B-Lunaris-v1-Q8_0_L.gguf?download=true\", \"chatml\"),\n    (\"Llama-3.1-Dark-Planet-8-Orbs 8B Q8_0\", \"https://huggingface.co/DavidAU/L3-Dark-Planet-8B-V2-Eight-Orbs-Of-Power-GGUF/resolve/main/L3-Dark-Planet-8B-V2-EOOP-D_AU-Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Llama-3.1-DarkIdol 8B Q8_0\", \"https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf?download=true\", \"chatml\"),\n    (\"Yodayo-Nephra 8B Q8_0\", \"https://huggingface.co/Marcus-Arcadius/nephra_v1.0-Q8_0-GGUF/resolve/main/nephra_v1.0-q8_0.gguf?download=true\", \"chatml\"),\n    (\"Gemma2-SPPO 9B Q6_K\", \"https://huggingface.co/mradermacher/Gemma-2-9B-It-SPPO-Iter3-i1-GGUF/resolve/main/Gemma-2-9B-It-SPPO-Iter3.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Tarnished\", \"https://huggingface.co/mradermacher/tarnished-9b-GGUF/resolve/main/tarnished-9b.Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Gemma2-Daybreak(9b)\", \"https://huggingface.co/mradermacher/gemma2-9B-daybreak-v0.5-i1-GGUF/resolve/main/gemma2-9B-daybreak-v0.5.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Gemma2-Sunfall(9b)\", \"https://huggingface.co/mradermacher/gemma2-9B-sunfall-v0.5.2-i1-GGUF/resolve/main/gemma2-9B-sunfall-v0.5.2.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Gemma2-Ataraxy(9b)\", \"https://huggingface.co/bartowski/Gemma-2-Ataraxy-9B-GGUF/resolve/main/Gemma-2-Ataraxy-9B-Q5_K_L.gguf?download=true\", \"chatml\"),\n    (\"Fimbulvetr2(11b)\", \"https://huggingface.co/Lewdiculous/Fimbulvetr-11B-v2-GGUF-IQ-Imatrix/resolve/main/Fimbulvetr-11B-v2-Q5_K_M-imat.gguf?download=true\", \"chatml\"),\n    (\"Fimbulvetr-Kuro-Lotus(11b)\", \"https://huggingface.co/saishf/Fimbulvetr-Kuro-Lotus-10.7B-GGUF/resolve/main/Fimbulvetr-Kuro-Lotus-10.7B-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Kaiju(11b)\", \"https://huggingface.co/Himitsui/Kaiju-11B-GGUF/resolve/main/Kaiju-11B.q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Fimbulvetr-Holodeck-Erebus-Westlake(11b)\", \"https://huggingface.co/PJMixers/Fimbulvetr-Holodeck-Erebus-Westlake-10.7B-GGUF/resolve/main/Fimbulvetr-Holodeck-Erebus-Westlake-10.7B-q4_K_S.gguf\", \"chatml\"),\n    (\"MoistralV3(11b)\", \"https://huggingface.co/TheDrummer/Moistral-11B-v3-GGUF/resolve/main/Moistral-11B-v3-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Lumimaid(12b)\", \"https://huggingface.co/mradermacher/Lumimaid-v0.2-12B-i1-GGUF/resolve/main/Lumimaid-v0.2-12B.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Mini-Magnum(12b)\", \"https://huggingface.co/InferenceIllusionist/mini-magnum-12b-v1.1-iMat-GGUF/resolve/main/mini-magnum-12b-v1.1-iMat-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Nemomix(12b)\", \"https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Celeste(12b)\", \"https://huggingface.co/QuantFactory/Celeste-12B-V1.6-GGUF/resolve/main/Celeste-12B-V1.6.Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Lyra(12b)\", \"https://huggingface.co/Lewdiculous/MN-12B-Lyra-v4-GGUF-IQ-Imatrix/resolve/main/MN-12B-Lyra-v4-Q6_K-imat.gguf?download=true\", \"chatml\"),\n    (\"Guns-and-roses(12b)\", \"https://huggingface.co/Reiterate3680/guns-and-roses-r1-GGUF/resolve/main/guns-and-roses-r1-Q6_K_L-imat.gguf?download=true\", \"chatml\"),\n    (\"Magnum(12b)\", \"https://huggingface.co/mradermacher/magnum-v4-12b-GGUF/resolve/main/magnum-v4-12b.Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Starcannon(12b)\", \"https://huggingface.co/mradermacher/MN-12B-Starcannon-v3-i1-GGUF/resolve/main/MN-12B-Starcannon-v3.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Rocinante(12b)\", \"https://huggingface.co/TheDrummer/UnslopNemo-12B-v3-GGUF/resolve/main/Rocinante-12B-v2g-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Chronos Gold(12b)\", \"https://huggingface.co/mradermacher/Chronos-Gold-12B-1.0-i1-GGUF/resolve/main/Chronos-Gold-12B-1.0.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"L3.1 OpenCrystal(12b)\", \"https://huggingface.co/mradermacher/OpenCrystal-12B-L3-i1-GGUF/resolve/main/OpenCrystal-12B-L3.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Mag-Mell(12b)\", \"https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Violet Twilight(12b)\", \"https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Halide(12b)\", \"https://huggingface.co/mradermacher/MN-Halide-12b-v1.0-i1-GGUF/resolve/main/MN-Halide-12b-v1.0.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Stellar Odyssy(12b)\", \"https://huggingface.co/mradermacher/Stellar-Odyssey-12b-v0.0-i1-GGUF/resolve/main/Stellar-Odyssey-12b-v0.0.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"MadMix(12b)\", \"https://huggingface.co/mradermacher/MadMix-Unleashed-12B-i1-GGUF/resolve/main/MadMix-Unleashed-12B.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"DarkPlanet(12b)\", \"https://huggingface.co/DavidAU/MN-Dark-Planet-TITAN-12B-GGUF/resolve/main/MN-Dark-Planet-TITAN-12B-D_AU-Q6_k.gguf?download=true\", \"chatml\"),\n    (\"UnslopNemoV4.1(12b)\", \"https://huggingface.co/TheDrummer/UnslopNemo-12B-v4.1-GGUF/resolve/main/Rocinante-12B-v2j-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Violet Lotus(12b)\", \"https://huggingface.co/QuantFactory/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Abomination Science(12b)\", \"https://huggingface.co/mradermacher/AbominationScience-12B-v4-i1-GGUF/resolve/main/AbominationScience-12B-v4.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"DarkAtom(12b)\", \"https://huggingface.co/mradermacher/DarkAtom-12B-v3-i1-GGUF/resolve/main/DarkAtom-12B-v3.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"CaptainErisViolet(12b)\", \"https://huggingface.co/QuantFactory/Captain-Eris_Violet-V0.420-12B-GGUF/resolve/main/Captain-Eris_Violet-V0.420-12B.Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Ink(12b)\", \"https://huggingface.co/allura-org/MN-12b-RP-Ink-GGUF/resolve/main/MN-12b-RP-Ink-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Wayfarer(12b)\", \"https://huggingface.co/LatitudeGames/Wayfarer-12B-GGUF/resolve/main/Wayfarer-12B-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"TieFighter(13b)\", \"https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/resolve/main/LLaMA2-13B-Tiefighter.Q4_K_S.gguf?download=true\", \"chatml\"),\n    (\"Psyfighter(13b)\", \"https://huggingface.co/TheBloke/Psyfighter-13B-GGUF/resolve/main/psyfighter-13b.Q5_K_M.gguf\", \"chatml\"),\n    (\"Psyfighter2(13b)\", \"https://huggingface.co/KoboldAI/LLaMA2-13B-Psyfighter2-GGUF/resolve/main/LLaMA2-13B-Psyfighter2.Q4_K_M.gguf\", \"chatml\"),\n    (\"PsyMedRP(13b)\", \"https://huggingface.co/Undi95/PsyMedRP-v1-13B-GGUF/resolve/main/PsyMedRP-v1-13B.q5_k_m.gguf\", \"chatml\"),\n    (\"EstopianMaid(13b)\", \"https://huggingface.co/KatyTheCutie/EstopianMaid-13B-GGUF/resolve/main/EstopianMaid-13B-Q4_K_S.gguf\", \"chatml\"),\n    (\"Noromaid0.1(13b)\", \"https://huggingface.co/NeverSleep/Noromaid-13b-v0.1.1-GGUF/resolve/main/Noromaid-13b-v0.1.1.q4_k_s.gguf\", \"chatml\"),\n    (\"EVA-Qwen2.5 (14b)\", \"https://huggingface.co/mradermacher/EVA-Qwen2.5-14B-v0.2-i1-GGUF/resolve/main/EVA-Qwen2.5-14B-v0.2.i1-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Eidolon(14b)\", \"https://huggingface.co/Lambent/Eidolon-v2.1-14B-Q4_K_M-GGUF/resolve/main/eidolon-v2.1-14b-q4_k_m.gguf?download=true\", \"chatml\"),\n    (\"EVA-Tissint(14b)\", \"https://huggingface.co/mradermacher/EVA-Tissint-v1.2-14B-i1-GGUF/resolve/main/EVA-Tissint-v1.2-14B.i1-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Sugarquill(14b)\", \"https://huggingface.co/Triangle104/TQ2.5-14B-Sugarquill-v1-Q5_K_M-GGUF/resolve/main/tq2.5-14b-sugarquill-v1-q5_k_m.gguf?download=true\", \"chatml\"),\n    (\"Freya(14b)\", \"https://huggingface.co/mradermacher/14B-Qwen2.5-Freya-x1-i1-GGUF/resolve/main/14B-Qwen2.5-Freya-x1.i1-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Kunou(14b)\", \"https://huggingface.co/mradermacher/14B-Qwen2.5-Kunou-v1-GGUF/resolve/main/14B-Qwen2.5-Kunou-v1.Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"Sailor2(14b)\", \"https://huggingface.co/mradermacher/Sailor2-14B-GGUF/resolve/main/Sailor2-14B.Q4_K_S.gguf?download=true\", \"chatml\"),\n    (\"Sailor2-Chat(14b)\", \"https://huggingface.co/mradermacher/Sailor2-14B-Chat-GGUF/resolve/main/Sailor2-14B-Chat.Q4_K_S.gguf?download=true\", \"chatml\"),\n    (\"Deepseek-Kunou(14b)\", \"https://huggingface.co/mradermacher/Deepseeker-Kunou-Qwen2.5-14b-i1-GGUF/resolve/main/Deepseeker-Kunou-Qwen2.5-14b.i1-Q5_K_M.gguf?download=true\", \"chatml\"),\n    (\"L3.1 OpenCrystal(15b)\", \"https://huggingface.co/mradermacher/OpenCrystal-15B-L3-v2-i1-GGUF/resolve/main/OpenCrystal-15B-L3-v2.i1-Q6_K.gguf?download=true\", \"chatml\"),\n    (\"Cydonia(22b)\", \"https://huggingface.co/MarsupialAI/Cydonia-22B-v1_iMat_GGUF/resolve/main/Cydonia-22B-v1_iQ4xs.gguf?download=true\", \"chatml\"),\n    (\"Magnum(22b)\", \"https://huggingface.co/mradermacher/magnum-v4-22b-i1-GGUF/resolve/main/magnum-v4-22b.i1-IQ4_XS.gguf?download=true\", \"chatml\"),\n    (\"Sorcerer(22b)\", \"https://huggingface.co/Quant-Cartel/SorcererLM-22B-iMat-GGUF/resolve/main/SorcererLM-22B-iMat-IQ4_XS.gguf?download=true\", \"chatml\"),\n]\n\n_dropdown_options = [(label, (url, instruct)) for (label, url, instruct) in model_options]\n\nmodel_dropdown = widgets.Dropdown(\n    options=_dropdown_options,\n    value=_dropdown_options[0][1],\n    description='Model:',\n    layout=widgets.Layout(width='100%')\n)\n\ncustom_url = widgets.Text(\n    value='',\n    placeholder='Or paste a custom GGUF URL here...',\n    description='Custom URL:',\n    layout=widgets.Layout(width='100%')\n)\n\nuse_custom = widgets.Checkbox(value=False, description='Use custom URL')\n\ncontext_size_input = widgets.IntText(\n    value=16384,\n    description='Context:',\n    layout=widgets.Layout(width='200px')\n)\n\nmax_tokens_input = widgets.IntText(\n    value=2048,\n    description='Max tokens:',\n    layout=widgets.Layout(width='200px')\n)\n\ninstruct_preset_dropdown = widgets.Dropdown(\n    options=list(premade_instruct.keys()),\n    value=\"chatml\",\n    description='Instruct Preset:',\n    layout=widgets.Layout(width='300px')\n)\n\nlayers_input = widgets.IntText(value=99, description='Layers:')\nkvcache_input = widgets.Dropdown(options=[\"0\",\"1\",\"2\"], value=\"0\", description='KvCache:')\nblayer_input = widgets.IntText(value=99, description='blayer:')\n\ndef get_selected_url():\n    return custom_url.value.strip() if use_custom.value and custom_url.value.strip() else model_dropdown.value[0]\n\ndef get_selected_instruct():\n    return instruct_preset_dropdown.value if use_custom.value else model_dropdown.value[1]\n\ndef _on_model_change(change):\n    url, instruct = change['new']\n    instruct_preset_dropdown.value = instruct\n\nmodel_dropdown.observe(_on_model_change, names='value')\n\n# Display UI (models / core settings)\nui_models = widgets.VBox([\n    model_dropdown,\n    widgets.HBox([use_custom, custom_url]),\n    widgets.HBox([context_size_input, max_tokens_input]),\n    widgets.HTML(value=\"<hr>\"),\n    instruct_preset_dropdown,\n    widgets.HTML(value=\"<hr>\"),\n    widgets.HTML(value=\"<span>Advanced settings: Do not tweak unless you know what you are doing.</span>\"),\n    widgets.HBox([layers_input, kvcache_input, blayer_input])\n])\n\ndisplay(ui_models)\n\n_SELECTED_URL_VALUE = get_selected_url()\n_CONTEXT_SIZE_VALUE = int(context_size_input.value)\n_MAX_TOKENS_VALUE = int(max_tokens_input.value)\n_INSTRUCT_PRESET_VALUE = get_selected_instruct()\n_LAYERS_VALUE = int(layers_input.value)\n_KVCACHE_VALUE = kvcache_input.value\n_BLAYER_VALUE = int(blayer_input.value)\n\ndef _on_any_change(change):\n    global _SELECTED_URL_VALUE, _CONTEXT_SIZE_VALUE, _MAX_TOKENS_VALUE\n    global _INSTRUCT_PRESET_VALUE\n    global _LAYERS_VALUE, _KVCACHE_VALUE, _BLAYER_VALUE\n    _SELECTED_URL_VALUE = get_selected_url()\n    _CONTEXT_SIZE_VALUE = int(context_size_input.value)\n    _MAX_TOKENS_VALUE = int(max_tokens_input.value)\n    _INSTRUCT_PRESET_VALUE = get_selected_instruct()\n    _LAYERS_VALUE = int(layers_input.value)\n    _KVCACHE_VALUE = kvcache_input.value\n    _BLAYER_VALUE = int(blayer_input.value)\n\nfor w in [model_dropdown, custom_url, use_custom,\n          context_size_input, max_tokens_input,\n          instruct_preset_dropdown,\n          layers_input, kvcache_input, blayer_input]:\n    w.observe(_on_any_change, names='value')\n\n\n# -------------------------------\n# Proxy + Sampler settings UI\n# -------------------------------\n\n# Managed internals (hidden from users)\n_PROXY_PORT = 5002\n_KOBOLD_BASE = 'http://127.0.0.1:5001'\n\n# Defaults aligned with jai-proxy-suite\n_instruct_from_notebook = globals().get('_INSTRUCT_PRESET_VALUE', 'chatml')\nweb_param = {\n    \"instruct\": _instruct_from_notebook,\n    \"top_p\": 0.92,\n    \"min_p\": 0.12,\n    \"top_k\": -1,\n    \"repetition_penalty\": 1.05,\n    \"frequency_penalty\": 0,\n    \"presence_penalty\": 0.26,\n    \"banned_strings\": [],\n    \"dry_enabled\": False,\n    \"dry_multiplier\": 1.75,\n    \"dry_base\": 1.1,\n    \"dry_allowed_length\": 3,\n    \"dry_range\": 1024,\n}\n\n# Sampler controls only\n_top = widgets.HBox([\n    widgets.FloatText(value=0.92, description='top_p:'),\n    widgets.FloatText(value=0.12, description='min_p:'),\n    widgets.IntText(value=-1, description='top_k:'),\n])\n_rep = widgets.HBox([\n    widgets.FloatText(value=1.05, description='rep_pen:'),\n    widgets.FloatText(value=0.0, description='freq_pen:'),\n    widgets.FloatText(value=0.26, description='pres_pen:'),\n])\n\nbanned_strings_input = widgets.Textarea(\n    value=\"\",\n    description='banned_strings:',\n    layout=widgets.Layout(width='100%', height='80px')\n)\n\ndry_enabled_chk = widgets.Checkbox(value=False, description='DRY enabled')\ndry_mult_input = widgets.FloatText(value=1.75, description='dry_mult:')\ndry_base_input = widgets.FloatText(value=1.1, description='dry_base:')\ndry_len_input = widgets.IntText(value=3, description='dry_len:')\ndry_range_input = widgets.IntText(value=1024, description='dry_range:')\n\ndef _sync_from_widgets(_=None):\n    # Always refresh instruct from the model section (first part of this cell)\n    web_param[\"instruct\"] = globals().get('_INSTRUCT_PRESET_VALUE', 'chatml')\n\n    # Map sampler inputs from UI containers\n    web_param[\"top_p\"] = float(_top.children[0].value)\n    web_param[\"min_p\"] = float(_top.children[1].value)\n    web_param[\"top_k\"] = int(_top.children[2].value)\n\n    web_param[\"repetition_penalty\"] = float(_rep.children[0].value)\n    web_param[\"frequency_penalty\"] = float(_rep.children[1].value)\n    web_param[\"presence_penalty\"] = float(_rep.children[2].value)\n\n    banned_raw = banned_strings_input.value.strip()\n    web_param[\"banned_strings\"] = [s.strip() for s in banned_raw.split(',')] if banned_raw else []\n\n    web_param[\"dry_enabled\"] = bool(dry_enabled_chk.value)\n    web_param[\"dry_multiplier\"] = float(dry_mult_input.value)\n    web_param[\"dry_base\"] = float(dry_base_input.value)\n    web_param[\"dry_allowed_length\"] = int(dry_len_input.value)\n    web_param[\"dry_range\"] = int(dry_range_input.value)\n\n# Attach observers to every relevant widget\nfor w in [\n    _top.children[0], _top.children[1], _top.children[2],\n    _rep.children[0], _rep.children[1], _rep.children[2],\n    banned_strings_input,\n    dry_enabled_chk, dry_mult_input, dry_base_input, dry_len_input, dry_range_input\n]:\n    w.observe(_sync_from_widgets, names='value')\n\n# Initial sync so web_param matches defaults immediately\n_sync_from_widgets()\n\nui_sampler = widgets.VBox([\n    widgets.HTML(value=\"<b>Sampler params</b>\"),\n    _top,\n    _rep,\n    banned_strings_input,\n    widgets.HTML(value=\"<hr><b>DRY Sampling</b>\"),\n    widgets.HBox([dry_enabled_chk, dry_mult_input, dry_base_input, dry_len_input, dry_range_input])\n])\n\ndisplay(ui_sampler)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-22T17:32:42.054191Z","iopub.execute_input":"2025-09-22T17:32:42.054492Z","iopub.status.idle":"2025-09-22T17:32:42.287033Z","shell.execute_reply.started":"2025-09-22T17:32:42.054466Z","shell.execute_reply":"2025-09-22T17:32:42.286417Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Dropdown(description='Model:', layout=Layout(width='100%'), options=(('Kunoichi 7B Q8_0', ('htt…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"808c235fd5cb4cc3aaae2a211997e785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<b>Sampler params</b>'), HBox(children=(FloatText(value=0.92, description='top_p:')…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfff3950183249a5a8bd167bf8ef8bd5"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"### Reverse proxy setup\n\n#### How to tell when this cell is done:\n\n`Proxy running on port 5002, forwarding to http://127.0.0.1:5001` should appear in the cell's output.","metadata":{}},{"cell_type":"code","source":"# Reverse proxy for Kobold\n!pip -q install flask flask_cors requests\nfrom flask import Flask, request, jsonify, Response, stream_with_context\nfrom flask_cors import CORS\nimport requests as _rq\nimport json as _json\nimport time as _time\n\napp = Flask(__name__)\nCORS(app)\n\n# defaults if UI not run\ntry:\n    _ = _PROXY_PORT\nexcept NameError:\n    _PROXY_PORT = 5002\ntry:\n    _ = _KOBOLD_BASE\nexcept NameError:\n    _KOBOLD_BASE = 'http://127.0.0.1:5001'\ntry:\n    _ = web_param\nexcept NameError:\n    web_param = {\n        \"instruct\": \"chatml\",\n        \"top_p\": 0.92,\n        \"min_p\": 0.12,\n        \"top_k\": -1,\n        \"repetition_penalty\": 1.05,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0.26,\n        \"banned_strings\": [],\n        \"dry_enabled\": False,\n        \"dry_multiplier\": 1.75,\n        \"dry_base\": 1.1,\n        \"dry_allowed_length\": 3,\n        \"dry_range\": 1024,\n    }\n\n# prompt adapter like jai-proxy-suite\ntry:\n    _ = premade_instruct\nexcept NameError:\n    premade_instruct = {\n        \"chatml\": {\n            \"system_start\": \"<|im_start|>system\",\n            \"system_end\": \"<|im_end|>\",\n            \"user_start\": \"<|im_start|>user\",\n            \"user_end\": \"<|im_end|>\",\n            \"assistant_start\": \"<|im_start|>assistant\",\n            \"assistant_end\": \"<|im_end|>\",\n        }\n    }\n\n\ndef message_instructor(messages_list, preset=None):\n    adapter_key = preset or web_param.get('instruct', 'chatml')\n    adapter = premade_instruct.get(adapter_key, premade_instruct['chatml'])\n    ss = adapter.get('system_start', '')\n    se = adapter.get('system_end', '')\n    us = adapter.get('user_start', '')\n    ue = adapter.get('user_end', '')\n    as_ = adapter.get('assistant_start', '')\n    ae = adapter.get('assistant_end', '')\n    out = []\n    for m in messages_list:\n        if m['role'] == 'system':\n            out.append(ss + m['content'] + se)\n        elif m['role'] == 'user':\n            out.append(us + m['content'] + ue)\n        elif m['role'] == 'assistant':\n            out.append(as_ + m['content'] + ae)\n        elif m['role'] == 'tool':\n            out.append(m['content'])\n    out.append(as_)\n    return ''.join(out)\n\n\ndef build_kobold_payload(src):\n    # Keep instruct preset in sync with the model selection UI\n    try:\n        web_param['instruct'] = globals().get('_INSTRUCT_PRESET_VALUE', web_param.get('instruct', 'chatml'))\n    except Exception:\n        pass\n\n    # Take OpenAI Chat Completions body and map to Kobold-compatible params\n    body = {\n        'model': src.get('model', ''),\n        'temperature': src.get('temperature', 0.9),\n        'max_tokens': src.get('max_tokens', 2048),\n        'min_p': web_param['min_p'],\n        'top_p': web_param['top_p'],\n        'top_k': web_param['top_k'],\n        'repetition_penalty': web_param['repetition_penalty'],\n        'presence_penalty': web_param['presence_penalty'],\n        'frequency_penalty': web_param['frequency_penalty'],\n        'banned_strings': web_param['banned_strings'],\n        'n': 1,\n        'best_of': 1,\n        'skip_special_tokens': True,\n        'sampler_order': [6,0,1,3,4,2,5]\n    }\n    # DRY sampling pass-through\n    if web_param.get('dry_enabled'):\n        body.update({\n            'dry_allowed_length': web_param['dry_allowed_length'],\n            'dry_base': web_param['dry_base'],\n            'dry_multiplier': web_param['dry_multiplier'],\n            'dry_penalty_last_n': web_param['dry_range'],\n            'dry_sequence_breakers': web_param.get('dry_sequence_breakers', None)\n        })\n    # Adapt messages to single prompt if backend expects that; Kobold v1 chat supports messages\n    if 'messages' in src:\n        body['messages'] = src['messages']\n    if src.get('messages') and web_param.get('instruct'):\n        # also build an instruct prompt for adapters that need it (some kobold variants)\n        body['prompt'] = message_instructor(src['messages'])\n    # stream flag\n    body['stream'] = bool(src.get('stream', False))\n    return body\n\n\ndef forward_stream(endpoint_url, config):\n    def gen():\n        try:\n            with _rq.post(endpoint_url, json=config, stream=True) as r:\n                r.raise_for_status()\n                for line in r.iter_lines():\n                    if not line:\n                        continue\n                    text = line.decode('utf-8')\n                    yield f\"{text}\\n\\n\"\n                    _time.sleep(0.02)\n        except Exception as e:\n            err = {\"error\": str(e)}\n            yield f\"data: {_json.dumps(err)}\\n\\n\"\n    return Response(stream_with_context(gen()), content_type='text/event-stream')\n\n\ndef forward_normal(endpoint_url, config):\n    try:\n        r = _rq.post(endpoint_url, json=config)\n        r.raise_for_status()\n        data = r.json()\n        # auto-trim like suite (simple sentence boundary trim)\n        try:\n            txt = data.get('choices', [{}])[0].get('message', {}).get('content')\n            if isinstance(txt, str):\n                data['choices'][0]['message']['content'] = txt.rstrip()\n        except Exception:\n            pass\n        return jsonify(data)\n    except Exception as e:\n        return Response(_json.dumps({\"error\": str(e)}), status=500, content_type='application/json')\n\n\n@app.route('/v1/chat/completions', methods=['POST'])\ndef proxy_chat():\n    if not request.json:\n        return jsonify(error=True), 400\n    endpoint = f\"{_KOBOLD_BASE}/v1/chat/completions\"\n    payload = build_kobold_payload(request.json)\n    if payload.get('stream'):\n        return forward_stream(endpoint, payload)\n    else:\n        return forward_normal(endpoint, payload)\n\n\nimport threading as _th\n\ndef _run_proxy():\n    app.run(host='0.0.0.0', port=_PROXY_PORT, use_reloader=False)\n\ntry:\n    PROXY_THREAD\n    # If already defined, do nothing or restart logic could be added here\nexcept NameError:\n    PROXY_THREAD = _th.Thread(target=_run_proxy, daemon=True)\n    PROXY_THREAD.start()\nprint(f\"Proxy running on port {_PROXY_PORT}, forwarding to {_KOBOLD_BASE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T16:51:36.814038Z","iopub.execute_input":"2025-09-22T16:51:36.814335Z","iopub.status.idle":"2025-09-22T16:51:45.382461Z","shell.execute_reply.started":"2025-09-22T16:51:36.814314Z","shell.execute_reply":"2025-09-22T16:51:45.38073Z"}},"outputs":[{"name":"stdout","text":"Proxy running on port 5002, forwarding to http://127.0.0.1:5001\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Download model\n\n#### How to tell when this cell is done:\n\nThe name of the model you selected should appear below, as well as a full download progress bar.","metadata":{}},{"cell_type":"code","source":"# Download selected model\nimport os\n!pip install hf_transfer\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\ntry:\n    URL = _SELECTED_URL_VALUE if '_SELECTED_URL_VALUE' in globals() and _SELECTED_URL_VALUE else URL\nexcept NameError:\n    URL = \"https://huggingface.co/bartowski/trashpanda-org_QwQ-32B-Snowdrop-v0-GGUF/resolve/main/trashpanda-org_QwQ-32B-Snowdrop-v0-Q4_K_L.gguf\"\n\nfrom urllib.parse import urlparse, parse_qs\n\ndef extract_parts(url):\n    parsed_url = urlparse(url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    query = parse_qs(parsed_url.query)\n    \n    model_name = path_parts[0]+\"/\"+path_parts[1]\n    version = path_parts[3]\n    file_name = path_parts[4]\n    \n    return model_name, version, file_name\n\nREPO_ID, REVISION, FILE = extract_parts(URL)\n\nDIR = REPO_ID.replace(\"/\", \"_\")\n\nif REVISION != \"\" and REVISION != \"main\":\n    DIR = f\"{DIR}_{REVISION}\"\n\nprint(\"Model Name: \" + DIR)\nprint(\"Version: \" + REVISION)\nprint(\"File: \" + FILE)\n\nfrom huggingface_hub import hf_hub_download\n\nFULLPATH = f\"/kaggle/models/{DIR}\"\n\nif REVISION != \"\":\n    hf_hub_download(repo_id=REPO_ID, filename=FILE, local_dir_use_symlinks=False, revision=REVISION, local_dir=FULLPATH)\n    \nif REVISION == \"\":\n    hf_hub_download(repo_id=REPO_ID, filename=FILE, local_dir_use_symlinks=False, local_dir=FULLPATH)","metadata":{"execution":{"iopub.status.busy":"2025-09-22T16:52:17.795475Z","iopub.execute_input":"2025-09-22T16:52:17.796392Z","iopub.status.idle":"2025-09-22T16:54:46.849975Z","shell.execute_reply.started":"2025-09-22T16:52:17.796361Z","shell.execute_reply":"2025-09-22T16:54:46.849039Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\nModel Name: bartowski_TheDrummer_Cydonia-24B-v2.1-GGUF\nVersion: main\nFile: TheDrummer_Cydonia-24B-v2.1-Q6_K_L.gguf\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"TheDrummer_Cydonia-24B-v2.1-Q6_K_L.gguf:   0%|          | 0.00/19.7G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4167cb13b9cf492a834a4bd7cbec5528"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### Expose endpoint via ngrok\n\n#### How to tell when this cell is done:\n\nA button containing `Copy API Link` should appear in the cell output.","metadata":{}},{"cell_type":"code","source":"# ngrok setup\n!pip3 install pyngrok\nfrom kaggle_secrets import UserSecretsClient\nfrom pyngrok import ngrok\nsecret_label = \"ngrok-auth\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!ngrok config add-authtoken {secret_value}\ntunnel = ngrok.connect(5002)\nprint(\"Your remote link is: \" + tunnel.public_url)\nfrom IPython.display import HTML\nhtml = f'''\n    <div>\n    <h4>\n    <a href=\"{tunnel.public_url}/v1\" id=\"api\">{tunnel.public_url}/v1</a>\n    <button onclick=\"copyToClipboard('api')\">Copy API Link</button>\n    </h4>\n    </div>\n    \n    <script>\n    function copyToClipboard(copy) {{\n        var textToCopy = document.getElementById(copy).innerText;\n        var tempTextarea = document.createElement(\"textarea\");\n        tempTextarea.value = textToCopy;\n        document.body.appendChild(tempTextarea);\n        tempTextarea.select();\n        tempTextarea.setSelectionRange(0, 99999);\n        document.execCommand(\"copy\");\n        document.body.removeChild(tempTextarea);\n        alert(\"Copied the text: \" + textToCopy);\n    }}\n    </script>\n'''\n\ndisplay(HTML(html))","metadata":{"execution":{"iopub.status.busy":"2025-09-22T16:57:20.783254Z","iopub.execute_input":"2025-09-22T16:57:20.783522Z","iopub.status.idle":"2025-09-22T16:57:26.617915Z","shell.execute_reply.started":"2025-09-22T16:57:20.783505Z","shell.execute_reply":"2025-09-22T16:57:26.617275Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\nDownloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.3.0\nAuthtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \nYour remote link is: https://0e57af551ce7.ngrok-free.app\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n    <h4>\n    <a href=\"https://0e57af551ce7.ngrok-free.app/v1\" id=\"api\">https://0e57af551ce7.ngrok-free.app/v1</a>\n    <button onclick=\"copyToClipboard('api')\">Copy API Link</button>\n    </h4>\n    </div>\n    \n    <script>\n    function copyToClipboard(copy) {\n        var textToCopy = document.getElementById(copy).innerText;\n        var tempTextarea = document.createElement(\"textarea\");\n        tempTextarea.value = textToCopy;\n        document.body.appendChild(tempTextarea);\n        tempTextarea.select();\n        tempTextarea.setSelectionRange(0, 99999);\n        document.execCommand(\"copy\");\n        document.body.removeChild(tempTextarea);\n        alert(\"Copied the text: \" + textToCopy);\n    }\n    </script>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### Launch Kobold\n\n#### How to tell when this cell is done:\n\n`Please connect to custom endpoint at http://localhost:5001` should appear in the cell output.","metadata":{}},{"cell_type":"code","source":"# Start Kobold\nMODEL = FULLPATH + \"/\" + FILE\nprint(\"Model to load: \" + MODEL)\n\nimport json as _json\n_preset = _INSTRUCT_PRESET_VALUE if '_INSTRUCT_PRESET_VALUE' in globals() else 'chatml'\n_tpl = premade_instruct.get(_preset, premade_instruct['chatml'])\nwith open('instruct.json', 'w') as f:\n    f.write(_json.dumps(_tpl, separators=(\",\", \":\")))\n\n_layers = _LAYERS_VALUE if '_LAYERS_VALUE' in globals() else 999\n\n!./koboldcpp {MODEL} --contextsize {_CONTEXT_SIZE_VALUE if '_CONTEXT_SIZE_VALUE' in globals() else 24000} --usecublas 0 1 normal mmq rowsplit --blasbatchsize 512 --flashattention --foreground --gpulayers {_layers} --quiet --threads 999 --blasthreads 999 --nommap --tensor_split 1 1 --skiplauncher --defaultgenamt={_MAX_TOKENS_VALUE if '_MAX_TOKENS_VALUE' in globals() else 2048} --chatcompletionsadapter instruct.json","metadata":{"execution":{"execution_failed":"2025-09-22T17:31:26.413Z"},"trusted":true},"outputs":[],"execution_count":null}]}